{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67552e06-f2bd-4fd4-907b-13b30c664f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import urlextract\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import emoji\n",
    "import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "import urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1957dd29-1967-4379-ac67-ea0f299018f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANING\n",
    "df = pd.read_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/MFRC.csv')\n",
    "df = df.drop(['confidence', 'annotator'], axis=1)\n",
    "\n",
    "# Split rows by posts that have numerous Moral Foundations attached to it, and save the data under a new name\n",
    "df = df.assign(annotation=df['annotation'].str.split(','))\n",
    "df = df.explode('annotation')\n",
    "df.to_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/MFRC_ind_anno.csv', index=False)\n",
    "\n",
    "# create a new DataFrame to store the majority annotated posts\n",
    "majority_annotated_posts = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# iterate through each unique post\n",
    "for unique_post in df['text'].unique():\n",
    "    # create a temporary DataFrame to store the duplicate rows for the current unique post\n",
    "    temp_df = df[df['text'] == unique_post].copy()\n",
    "    \n",
    "    # get the count of each annotation for the current post\n",
    "    annotation_counts = temp_df['annotation'].value_counts()\n",
    "    \n",
    "    # get the highest count of annotations for the current post\n",
    "    max_annotation_count = annotation_counts.max()\n",
    "    \n",
    "    # check if there is a tie for the highest count of annotations\n",
    "    if (annotation_counts == max_annotation_count).sum() == 1:\n",
    "        # get the majority annotation for the current post\n",
    "        majority_annotation = annotation_counts.idxmax()\n",
    "\n",
    "        # create a new row with the majority annotation and add it to the majority_annotated_posts DataFrame\n",
    "        new_row = temp_df.iloc[0].copy()\n",
    "        new_row['annotation'] = majority_annotation\n",
    "        majority_annotated_posts = pd.concat([majority_annotated_posts, new_row.to_frame().T], ignore_index=True)\n",
    "    else:\n",
    "        # there is a tie for the highest count of annotations\n",
    "        for annotation, count in annotation_counts.items():\n",
    "            if count == max_annotation_count:\n",
    "                # create a new row for each annotation that has the highest count and add it to the majority_annotated_posts DataFrame\n",
    "                new_row = temp_df[temp_df['annotation'] == annotation].iloc[0].copy()\n",
    "                majority_annotated_posts = pd.concat([majority_annotated_posts, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Save in new file\n",
    "majority_annotated_posts.to_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/majority_annotated_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52081dcb-bc89-4b87-83b5-8642df1f0c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/majority_annotated_posts.csv')\n",
    "\n",
    "# Exclude rows where 'annotation' column has 'Non-Moral' or 'Thin Morality'\n",
    "df = df[~df['annotation'].isin(['Non-Moral', 'Thin Morality'])]\n",
    "\n",
    "# Drop cleaning tools\n",
    "only_foundations_df = df.drop(['post_ID', 'row_ID'], axis=1)\n",
    "only_foundations_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ccdc6a-6c54-44c2-be31-8f9468e727ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for cleaning the posts\n",
    "def clean_text(text):\n",
    "    # Remove URLs using urlextract\n",
    "    extractor = urlextract.URLExtract()\n",
    "    urls = extractor.find_urls(text)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, '')\n",
    "\n",
    "    # Remove non-ASCII characters using unidecode\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remove extra white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Spell check\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words]\n",
    "    text = ' '.join(corrected_words)\n",
    "\n",
    "    return text\n",
    "\n",
    "only_foundations_df['text'] = only_foundations_df['text'].apply(clean_text)\n",
    "\n",
    "# Save in new file\n",
    "only_foundations_df.to_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/MFRC_clean_for_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75b84ab7-5af1-4b3b-bfcc-a7932a7481be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run sentiment analysis\n",
    "\n",
    "# Open up file\n",
    "df = pd.read_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/MFRC_clean_for_analysis.csv')\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Run sentiment analysis on each post store results in new column called 'sentiment_scores'\n",
    "sentiment = []\n",
    "for index, row in df.iterrows():\n",
    "    sentiment_scores = sid.polarity_scores(row['text'])\n",
    "    row['sentiment_scores'] = sentiment_scores\n",
    "    sentiment.append(row)\n",
    "\n",
    "# Convert  sentiment list back to PANDAS DataFrame\n",
    "df_with_sentiment = pd.DataFrame(sentiment)\n",
    "\n",
    "### Now to get expand the dictionary in the sentiment_scores into respective columns for individual analysis\n",
    "# expand the dictionary column into separate columns\n",
    "sentiment_normalize = pd.json_normalize(df_with_sentiment['sentiment_scores'])\n",
    "\n",
    "# merge the expanded columns with the original dataframe\n",
    "final_df = pd.concat([df_with_sentiment, sentiment_normalize], axis=1)\n",
    "\n",
    "# Save in new file. This file has duplicates of posts with different foundations ascribed to it in case of vote ties\n",
    "final_df.to_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/DF_with_foundation_duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e905cb31-875d-4232-802d-4a49f8e74f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Lose all rows which have 'split-ties' over relevant Moral Foundation\n",
    "df = pd.read_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/DF_with_foundation_duplicates.csv')\n",
    "df = df[~df['text'].duplicated(keep=False)]\n",
    "# Reset the index\n",
    "df = df.reset_index()\n",
    "# Save in new file\n",
    "df.to_csv('C:/Users/david/OneDrive/Skrivebord/Reddit Analysis/DF_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb213a-f86d-45cb-9544-b0302ed5d612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
